{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from BalticRiverPrediction.BaltNet import BaltNet\n",
    "from BalticRiverPrediction.BaltNet import LightningModel\n",
    "from BalticRiverPrediction.BaltNet import AtmosphereDataModule\n",
    "from BalticRiverPrediction.sharedUtilities import read_netcdfs, preprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from BalticRiverPrediction.convLSTM import ConvLSTM\n",
    "from BalticRiverPrediction.sharedUtilities import read_netcdfs, preprocess, plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n",
      "100%|██████████| 54/54 [00:50<00:00,  1.07it/s]\n",
      "100%|██████████| 97/97 [03:41<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set seed for reproducibility   \n",
    "L.seed_everything(123)\n",
    "\n",
    "# Our GPU has tensor cores, hence mixed precision training is enabled\n",
    "# see https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "datapath=\"/silor/boergel/paper/runoff_prediction/data\"\n",
    "\n",
    "data = read_netcdfs(\n",
    "    files=f\"{datapath}/atmosphericForcing/????/rain.mom.dta.nc\",\n",
    "    dim=\"time\",\n",
    "    transform_func=lambda ds:preprocess(ds)\n",
    "    ) \n",
    "\n",
    "# data = read_netcdfs(\n",
    "#     files=f\"{datapath}/atmosphericForcing/????/shumi.mom.dta.nc\",\n",
    "#     dim=\"time\",\n",
    "#     transform_func=lambda ds:preprocess(ds)\n",
    "#     )             \n",
    "\n",
    "runoff = read_netcdfs(\n",
    "    f\"{datapath}/runoffData/combined_fastriver_*.nc\",\n",
    "    dim=\"river\",\n",
    "    transform_func= lambda ds:ds.sel(time=slice(str(1979), str(2011))).roflux.resample(time=\"1D\").mean(),\n",
    "    cftime=False\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"lon_bnds\", \"lat_bnds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(\n",
    "    {\n",
    "        \"x\":\"lon\",\n",
    "        \"y\":\"lat\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AtmosphericDataset(Dataset):\n",
    "    def __init__(self, input_size, atmosphericData, runoff, transform=None):\n",
    "\n",
    "        # Update 6.10.2023\n",
    "        # The function is not handling the preprocessing anymore\n",
    "        # which makes the function more flexible \n",
    "        # Following the technical description of the river data (Germo et al.)\n",
    "        # the original river data is limited to 1979 to 2011\n",
    "        # start_year, end_year = 1979, 2011\n",
    "        # self.timeRange = slice(str(start_year), str(end_year))\n",
    "        \n",
    "        # Length of the sequence\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # input data (x) \n",
    "        atmosphericDataStd = atmosphericData.std(\"time\") # dimension will be channel, lat, lon\n",
    "        atmosphericDataMean = atmosphericData.mean(\"time\")\n",
    "        self.atmosphericStats = (atmosphericDataMean, atmosphericDataStd)\n",
    "\n",
    "        # output data - label (y)\n",
    "        runoffData = runoff.transpose(\"time\", \"river\")\n",
    "        runoffDataMean = runoffData.mean(\"time\")\n",
    "        runoffDataSTD = runoffData.std(\"time\")\n",
    "        self.runoffDataStats = (runoffDataMean, runoffDataSTD)\n",
    "\n",
    "        # save data\n",
    "        np.savetxt(\n",
    "            \"/silor/boergel/paper/runoff_prediction/data/modelStats.txt\",\n",
    "            [runoffDataMean, runoffDataSTD]\n",
    "        )\n",
    "        \n",
    "        # normalize data\n",
    "        X = ((atmosphericData - atmosphericDataMean)/atmosphericDataStd).compute()\n",
    "        y = ((runoffData - runoffDataMean)/runoffDataSTD).compute()\n",
    "        \n",
    "        # If only 3 dimension are available (time, lat, lon) \n",
    "        # an additional dimension for the channel is added\n",
    "        # to end up with (time, channel, lat, lon)\n",
    "\n",
    "        xStacked = X.to_array(dim='variable')\n",
    "        xStacked = xStacked.transpose(\"time\", \"variable\", \"lat\", \"lon\")\n",
    "\n",
    "        # if len(xStacked.data.ndim) == 3:\n",
    "        #     self.x = torch.tensor(xStacked.data, dtype=torch.float32).unsqueeze(dim=1)\n",
    "        # else:\n",
    "        assert xStacked.data.ndim == 4\n",
    "        self.x = torch.tensor(xStacked.data, dtype=torch.float16)\n",
    "        self.y = torch.tensor(y.data, dtype=torch.float16)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index:index+(self.input_size)], self.y[index+int(self.input_size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]-(self.input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AtmosphereDataModule(L.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, atmosphericData, runoff, batch_size=64, num_workers=8, add_first_dim=True, input_size=30):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = atmosphericData\n",
    "        self.runoff = runoff\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.add_first_dim = add_first_dim\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def setup(self, stage:str):\n",
    "        UserWarning(\"Loading atmospheric data ...\")\n",
    "        dataset = AtmosphericDataset(\n",
    "            atmosphericData=self.data,\n",
    "            runoff=self.runoff,\n",
    "            input_size=self.input_size\n",
    "            )\n",
    "        n_samples = len(dataset)\n",
    "        train_size = int(0.9 * n_samples)\n",
    "        val_size = n_samples - train_size\n",
    "        self.train, self.val, = random_split(dataset, [train_size, val_size])\n",
    "        # val_size = int(0.1 * n_samples)\n",
    "        # test_size = n_samples - train_size  - val_size\n",
    "        # self.train, self.val, self.test = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True, \n",
    "            drop_last=True, \n",
    "            num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            drop_last=True)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(\n",
    "    #         self.test,\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         shuffle=False,\n",
    "    #         num_workers=self.num_workers, \n",
    "    #         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaltNet(nn.Module):\n",
    "    def __init__(self, modelPar):\n",
    "        super(BaltNet, self).__init__()\n",
    "\n",
    "        # initialize all attributes\n",
    "        for k, v in modelPar.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.linear_dim = self.dimensions[0]*self.dimensions[1]*self.hidden_dim\n",
    "\n",
    "        self.convLSTM = ConvLSTM(\n",
    "                input_dim=self.input_dim,\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                kernel_size=self.kernel_size,\n",
    "                num_layers=self.num_layers,\n",
    "                batch_first=self.batch_first,\n",
    "                bias=self.bias,\n",
    "                return_all_layers=self.return_all_layers\n",
    "        )\n",
    "\n",
    "        self.convLSTM2 = ConvLSTM(\n",
    "                input_dim=self.input_dim,\n",
    "                hidden_dim=self.hidden_dim,\n",
    "                kernel_size=self.kernel_size,\n",
    "                num_layers=1,\n",
    "                batch_first=self.batch_first,\n",
    "                bias=self.bias,\n",
    "                return_all_layers=self.return_all_layers\n",
    "        )\n",
    "\n",
    "        # CNN layers to map the output of convLSTM2 to 97 rivers\n",
    "        # self.cnn_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(self.hidden_dim, 32, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.AdaptiveAvgPool2d((1, 1)),  # Global Average Pooling\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(32, 97)\n",
    "        # )\n",
    "\n",
    "        # CNN layers to map the output of convLSTM2 to 97 rivers\n",
    "        # self.cnn_layers = nn.Sequential(\n",
    "        #     nn.Conv2d(self.hidden_dim, 128, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        #     nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(256 * (self.dimensions[0] // 4) * (self.dimensions[1] // 4), 97)\n",
    "        # )\n",
    "        \n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.linear_dim, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.Linear(512, 256),\n",
    "            # torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 97)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, encode_state = self.convLSTM(x)\n",
    "        decoder_out, _ = self.convLSTM2(x[:,-1,:,:,:].unsqueeze(dim=1), encode_state)\n",
    "        x = decoder_out[0].squeeze(1)\n",
    "        # x = self.cnn_layers(x).squeeze()\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc_layers(x).squeeze()\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParameters = {\n",
    "\"input_dim\": 1, # Number of channel, right now only precipitation\n",
    "\"hidden_dim\": 8, # hidden states\n",
    "\"kernel_size\":(5,5), # applied for spatial convolutions\n",
    "\"num_layers\": 3, # number of convLSTM layers\n",
    "\"batch_first\":True, # first index is batch\n",
    "\"bias\":True, \n",
    "\"return_all_layers\": False, \n",
    "\"dimensions\": (191, 206) # dimensions of atmospheric forcing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[rank: 0] Global seed set to 123\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[rank: 1] Global seed set to 123\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/silos/conda_packages/boergel/miniconda3_4.12.0/OS_15.4/conda_env/BaltNet/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /silor/boergel/paper/runoff_prediction/data/modelWeights exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | BaltNet          | 40.3 M\n",
      "1 | train_mse | MeanSquaredError | 0     \n",
      "2 | val_mse   | MeanSquaredError | 0     \n",
      "3 | test_mse  | MeanSquaredError | 0     \n",
      "-----------------------------------------------\n",
      "40.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "40.3 M    Total params\n",
      "161.304   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ffe4a07e384fa6a7c0009ce4efd0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97a9cbc696440418a818b56cf1b3c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff6d9695532422bbe91a00f4815dc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481dc3f6de004c39bc598866ed3041ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd65e810e5414f688590136bcfc84e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a65e7d69f84d0c97ff7ab1b72a0ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176303426dfb40beb6648e9613668a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "### Setup model\n",
    "\n",
    "# Loads the atmospheric data in batches\n",
    "dataLoader = AtmosphereDataModule(\n",
    "atmosphericData=data,\n",
    "runoff=runoff,\n",
    "batch_size=64,\n",
    "input_size=30\n",
    ")\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# initalize model\n",
    "pyTorchBaltNet = BaltNet(modelPar=modelParameters)\n",
    "\n",
    "# Lightning model wrapper\n",
    "LighningBaltNet = LightningModel(\n",
    "    pyTorchBaltNet,\n",
    "    learning_rate=1e-3,\n",
    "    cosine_t_max=40\n",
    ")\n",
    "\n",
    "# save best model \n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=\"/silor/boergel/paper/runoff_prediction/data/modelWeights/\",\n",
    "        filename=\"BaltNetTopOne\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        monitor=\"val_mse\",\n",
    "        save_last=True\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"cuda\",\n",
    "    devices=2,\n",
    "    logger=CSVLogger(\n",
    "        save_dir=\"/silor/boergel/paper/runoff_prediction/logs\",\n",
    "        name=\"BaltNet1\"\n",
    "    ),\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model=LighningBaltNet, datamodule=dataLoader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
