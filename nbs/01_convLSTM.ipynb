{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp convLSTM\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Long-Shortterm Memory Network (ConvLSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code represent an implementation of the model structure suggested by Shi et al. (2015) - Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting. The idea is by extending the fully connected LSTM (FC-LSTM) with convolutional strucutres in both the input-to-state and state-to-state transitions (input and hidden state) which is named convolutional LSTM (convLSTM).\n",
    "\n",
    "## The model\n",
    "\n",
    "Although the FC-LSTM has proven powerpful for handling temporal correlations, it contains to much redundancy for spatial data. To adresse that the author propose to include convolutional strucutres. By stacking multiple ConvLSTM layers they were able to predit spatiotemporal sequences. The **major drawback** of FC-LSTM in handling spatiotemporal data is its usage of full connections in input to state and state-to-state transitions in which **no** spatial informations is encoded\n",
    "\n",
    "The ConvLSTM determines the future state of a certain cell in the grid by the inputs and past states of its local neighbors. This can easily be achieved by using a convolution operator in the state-to-state and input-to-state transitions.\n",
    "\n",
    "Here are the key equations where `*` denotes the convolutional operator and $\\circ$ as before the Hadamard product:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_{xi}*X_t+W_{hi}*H_{t-1}+W_{ci}\\circ C_{t-1}+bi) \\\\\n",
    "f_t = \\sigma(W_{xf}*X_t+W_{hf}*H_{t-1}+W_{cf}\\circ C_{t-1}+bf) \\\\\n",
    "C_t = f_t \\circ C_{t-1} + i_t \\circ tanh(W_{xc}*X_t+W_{hc}* H_{t-1}+bc) \\\\\n",
    "o_t = \\sigma(W_{xo}*X_t+W_{ho}*H_{t-1}+W_{co}\\circ C_{t-1}+bo) \\\\\n",
    "H_t = o_t \\circ tanh (C_t)\n",
    "$$\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:942/1*u8neecA4w6b_F1NgnyPP0Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ConvLSTMCell\n",
       "\n",
       ">      ConvLSTMCell (input_dim, hidden_dim, kernel_size, bias)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ConvLSTMCell\n",
       "\n",
       ">      ConvLSTMCell (input_dim, hidden_dim, kernel_size, bias)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ConvLSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False, stateful=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "        self.last_state = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        if hidden_state is None:\n",
    "            # Initialize hidden state if it's the first call or if not stateful\n",
    "            if not self.stateful or self.last_state is None:\n",
    "                hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\n",
    "            else:\n",
    "                # If the model is stateful and last_state is not None, use last_state\n",
    "                hidden_state = self.last_state\n",
    "        else:\n",
    "            # If hidden_state was provided as an input, we use it directly\n",
    "            self.last_state = hidden_state\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            # Fetching the hidden state for the current layer\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](\n",
    "                    input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                    cur_state=[h, c]\n",
    "                )\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append((h, c))  # Save the last state as a tuple for consistency\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = [layer_output_list[-1]]\n",
    "            last_state_list = [last_state_list[-1]]\n",
    "\n",
    "        if self.stateful:\n",
    "            self.last_state = last_state_list  # Save the last state for the next call\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ConvLSTM\n",
       "\n",
       ">      ConvLSTM (input_dim, hidden_dim, kernel_size, num_layers,\n",
       ">                batch_first=False, bias=True, return_all_layers=False)\n",
       "\n",
       "Parameters:\n",
       "    input_dim: Number of channels in input\n",
       "    hidden_dim: Number of hidden channels\n",
       "    kernel_size: Size of kernel in convolutions\n",
       "    num_layers: Number of LSTM layers stacked on each other\n",
       "    batch_first: Whether or not dimension 0 is the batch or not\n",
       "    bias: Bias or no bias in Convolution\n",
       "    return_all_layers: Return the list of computations for all layers\n",
       "    Note: Will do same padding.\n",
       "\n",
       "Input:\n",
       "    A tensor of size B, T, C, H, W or T, B, C, H, W\n",
       "Output:\n",
       "    A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
       "        0 - layer_output_list is the list of lists of length T of each output\n",
       "        1 - last_state_list is the list of last states\n",
       "                each element of the list is a tuple (h, c) for hidden state and memory\n",
       "Example:\n",
       "    >> x = torch.rand((32, 10, 64, 128, 128))\n",
       "    >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
       "    >> _, last_states = convlstm(x)\n",
       "    >> h = last_states[0][0]  # 0 for layer index, 0 for h index"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ConvLSTM\n",
       "\n",
       ">      ConvLSTM (input_dim, hidden_dim, kernel_size, num_layers,\n",
       ">                batch_first=False, bias=True, return_all_layers=False)\n",
       "\n",
       "Parameters:\n",
       "    input_dim: Number of channels in input\n",
       "    hidden_dim: Number of hidden channels\n",
       "    kernel_size: Size of kernel in convolutions\n",
       "    num_layers: Number of LSTM layers stacked on each other\n",
       "    batch_first: Whether or not dimension 0 is the batch or not\n",
       "    bias: Bias or no bias in Convolution\n",
       "    return_all_layers: Return the list of computations for all layers\n",
       "    Note: Will do same padding.\n",
       "\n",
       "Input:\n",
       "    A tensor of size B, T, C, H, W or T, B, C, H, W\n",
       "Output:\n",
       "    A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
       "        0 - layer_output_list is the list of lists of length T of each output\n",
       "        1 - last_state_list is the list of last states\n",
       "                each element of the list is a tuple (h, c) for hidden state and memory\n",
       "Example:\n",
       "    >> x = torch.rand((32, 10, 64, 128, 128))\n",
       "    >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
       "    >> _, last_states = convlstm(x)\n",
       "    >> h = last_states[0][0]  # 0 for layer index, 0 for h index"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ConvLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((32, 10, 1, 128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "convlstm = ConvLSTM(1, 16, (3,3), 1, batch_first=True, bias=True, return_all_layers=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convlstm.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, last_states = convlstm(x)\n",
    "h = last_states[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16, 128, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
