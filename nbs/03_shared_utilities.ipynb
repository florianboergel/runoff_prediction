{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sharedUtilities\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared utilties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def preprocess(ds):\n",
    "    \"\"\"\n",
    "    Used data has overlapping time data since they contain one month before the actual year\n",
    "    and one month after the actual year.\n",
    "    The function selects only the year of interest without the overlapping time data.\n",
    "\n",
    "    Args:\n",
    "        ds (xr.Dataset): netCDF4 data\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: data without time overlap\n",
    "    \"\"\"\n",
    "    return ds.sel(time=str(ds.time[len(ds.time)//2].dt.year.data)).resample(time=\"1D\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_netcdfs(files, dim, transform_func, transform_calendar=None, cftime = True):\n",
    "    \"\"\"Reads multiples netcdfs files\"\"\"\n",
    "    def process_one_path(path):\n",
    "        try:\n",
    "            if transform_calendar is not None:\n",
    "                calendar = False\n",
    "            else:\n",
    "                calendar = True\n",
    "            with xr.open_dataset(path, decode_times = calendar, use_cftime = cftime) as ds:\n",
    "                if transform_calendar is not None:\n",
    "                    ds[dim].attrs['calendar'] = transform_calendar\n",
    "                    ds = xr.decode_cf(ds, use_cftime = cftime)\n",
    "                if transform_func is not None:\n",
    "                    ds = transform_func(ds)\n",
    "                ds.load()\n",
    "                return ds\n",
    "        except Exception as e:\n",
    "            print(path)\n",
    "            print(e)\n",
    "            return\n",
    "    paths = sorted(glob(files))\n",
    "    datasets = [process_one_path(p) for p in tqdm(paths)]\n",
    "    datasets = [x for x in datasets if x is not None]\n",
    "    combined = xr.concat(datasets, dim)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def transform_calendar(ds,\n",
    "                       timedim=\"time\",\n",
    "                       calendarname=\"proleptic_gregorin\"):\n",
    "    \"\"\"Transforms calendar of time index in xarray dataset\"\"\"\n",
    "    ds[timedim].attrs['calendar'] = calendarname\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TaylorDiagram(object):\n",
    "    \"\"\"\n",
    "    Taylor diagram.\n",
    "    Plot model standard deviation and correlation to reference (data)\n",
    "    sample in a single-quadrant polar plot, with r=stddev and\n",
    "    theta=arccos(correlation).\n",
    "    \"\"\"\n",
    "    def __init__(self, refdata, ax,  *args, **kwargs):\n",
    "        # memorize reference data\n",
    "        self.refdata = refdata\n",
    "        self.ax = ax\n",
    "        refstd = refdata.std(ddof=1) \n",
    "        rlim = [0.0, 1.2*refstd]\n",
    "        thetalim = [0.0, 0.5*np.pi]\n",
    "        if ax != None:\n",
    "            # Add reference point and stddev contour\n",
    "            try:\n",
    "                kwargs[\"marker\"]\n",
    "            except:\n",
    "                kwargs[\"marker\"] = 'o'\n",
    "            try:\n",
    "                kwargs[\"color\"]\n",
    "            except:\n",
    "                kwargs[\"color\"] = \"black\"\n",
    "            try:\n",
    "                kwargs[\"ms\"]\n",
    "            except:\n",
    "                kwargs[\"ms\"] = 10     \n",
    "            try:\n",
    "                kwargs[\"label\"]\n",
    "            except:\n",
    "                kwargs[\"label\"] = \"reference\"                     \n",
    "            ax.plot([0], refstd, *args, ls='', zorder=0, **kwargs)\n",
    "            ax.grid(True, linestyle=\"--\")\n",
    "            \n",
    "            ax.set_xlabel(\"St. dev.\")\n",
    "            ax.xaxis.set_label_coords(0.5, -0.1)\n",
    "            ax.set_xlim(thetalim)\n",
    "            ax.set_ylim(rlim)\n",
    "            self.rlim = rlim\n",
    "            self.thetalim = thetalim\n",
    "        self.model_std = {\"reference\" : refstd}\n",
    "        self.corrcoef = {\"reference\" : 1.0}\n",
    "        self.rms = {\"reference\" : 0.0}\n",
    "    def add_sample(self, model_data, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Add sample to the Taylor\n",
    "        diagram. *args* and *kwargs* are directly propagated to the\n",
    "        `Figure.plot` command.\n",
    "        \"\"\"\n",
    "        model_std = model_data.std(ddof=1)\n",
    "        corrcoef = np.corrcoef(model_data, self.refdata)[0, 1]\n",
    "        rms = np.sqrt(self.model_std[\"reference\"]**2 + model_std**2 - 2.0*self.model_std[\"reference\"]*model_std*np.cos(np.arccos(corrcoef)))\n",
    "        try:\n",
    "            label = kwargs[\"label\"]\n",
    "        except:\n",
    "            label = \"model\"+str(len(self.model_std.keys())) \n",
    "        if self.ax != None:\n",
    "            self.ax.plot(np.arccos(corrcoef), model_std,\n",
    "                            *args, **kwargs)  # (theta, radius)\n",
    "            if model_std > self.rlim[1]:\n",
    "                self.rlim = [0, 1.2 * model_std]\n",
    "                self.ax.set_ylim(*self.rlim)\n",
    "            if np.arccos(corrcoef) > self.thetalim[1]:\n",
    "                self.thetalim[1] = 1.2 * np.arccos(corrcoef)\n",
    "                self.ax.set_xlim(*self.thetalim)                       \n",
    "        self.model_std[label] = model_std\n",
    "        self.corrcoef[label] = corrcoef\n",
    "        self.rms[label] = rms\n",
    "        #self.ax.set_ylim(rlim) \n",
    "    def get_samples(self):\n",
    "        return self.model_std, self.corrcoef, self.rms\n",
    "    def finalize(self):\n",
    "        \n",
    "        if self.ax is None:\n",
    "            return \n",
    "        self.ax.text(0.5*(self.thetalim[1]-self.thetalim[0]), (1.0+0.03*(self.thetalim[1]-self.thetalim[0])**2)*self.rlim[1],\"Correlation\", rotation=0.5*(self.thetalim[1]-self.thetalim[0])*180.0/np.pi-90.0)\n",
    "        rs, ts = np.meshgrid(np.linspace(*self.rlim), np.linspace(*self.thetalim))\n",
    "        rms = np.sqrt(self.model_std[\"reference\"]**2 + rs**2 - 2.0*self.model_std[\"reference\"]*rs*np.cos(ts))\n",
    "        self.ax.contourf(ts, rs, rms, 9, alpha=0.3, cmap=\"RdYlGn_r\")\n",
    "        contours = self.ax.contour(ts, rs, rms, 9, linestyles=\"--\", linewidths=1, alpha=0.5, colors=\"grey\")\n",
    "        if self.model_std[\"reference\"] > 100.0 or self.model_std[\"reference\"] < 0.1:\n",
    "            fmt = '%3.2e'\n",
    "        else:\n",
    "            fmt = '%3.2f'\n",
    "        self.ax.clabel(contours, inline=False, fmt=fmt, colors=\"black\")\n",
    "        t = np.linspace(*self.thetalim)\n",
    "        r = np.zeros_like(t) + self.model_std[\"reference\"]\n",
    "        self.ax.plot(t, r, 'k--', label='_')\n",
    "        xticks = [1.0, 0.99, 0.95, 0.9, 0.8, 0.7, 0.6, 0.4, 0.2, 0.0]\n",
    "        if self.thetalim[1] > 0.5*np.pi:\n",
    "            for x in [-0.2, -0.4, -0.6, -0.8, -0.9, -0.95, -0.99 -1.0]:\n",
    "                if np.arccos(x) > self.thetalim[1]:\n",
    "                    break\n",
    "                xticks += [x]\n",
    "        self.ax.set_xticks(np.arccos(xticks))\n",
    "        self.ax.set_xticklabels(xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EnhancedMSELoss(nn.Module):\n",
    "    def __init__(self, alpha=1.5):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced MSE loss module.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): Exponential factor to increase penalty for larger errors.\n",
    "        \"\"\"\n",
    "        super(EnhancedMSELoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Calculate the enhanced MSE loss.\n",
    "\n",
    "        Args:\n",
    "            predictions (torch.Tensor): The predicted values.\n",
    "            targets (torch.Tensor): The ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The calculated loss.\n",
    "        \"\"\"\n",
    "        error = predictions - targets\n",
    "        mse_loss = torch.mean(error**2)\n",
    "        enhanced_error = torch.mean(torch.abs(error) ** self.alpha)\n",
    "        enhanced_mse_loss = mse_loss + enhanced_error\n",
    "        return enhanced_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EnhancedMSEMetric(Metric):\n",
    "    def __init__(self, alpha=1.5, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.alpha = alpha\n",
    "        self.add_state(\"sum_enhanced_error\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, predictions: torch.Tensor, targets: torch.Tensor):\n",
    "        error = predictions - targets\n",
    "        mse_loss = torch.mean(error ** 2)\n",
    "        enhanced_error = torch.mean(torch.abs(error) ** self.alpha)\n",
    "\n",
    "        self.sum_enhanced_error += (mse_loss + enhanced_error) * targets.numel()\n",
    "        self.total += targets.numel()\n",
    "\n",
    "    def compute(self):\n",
    "        return self.sum_enhanced_error / self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PredictionPlottingCallback(L.Callback):\n",
    "    def __init__(self, num_samples_to_plot=10, rolling_window_size=120):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            testDataSet (Dataset): DataLoader for the prediction dataset.\n",
    "            num_samples_to_plot (int): Number of samples to plot.\n",
    "            dimensions_to_plot (tuple): Dimensions to plot in the time series.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.predictionDataLoader = DataLoader(testDataSet, batch_size=32, shuffle=False, drop_last=True)\n",
    "        self.num_samples_to_plot = num_samples_to_plot\n",
    "        self.rolling_window_size = rolling_window_size\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        test_dataloader = DataLoader(trainer.datamodule.test, batch_size=32, shuffle=False, drop_last=True)\n",
    "        try:\n",
    "            pl_module.eval()\n",
    "            self.all_predictions = []\n",
    "            self.all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_dataloader:\n",
    "                    features, true_labels = batch\n",
    "                    # Move data to the same device as the model\n",
    "                    features, true_labels = features.to(pl_module.device), true_labels.to(pl_module.device)\n",
    "                    predictions = pl_module(features)\n",
    "\n",
    "                    # Move predictions and labels to CPU for concatenation\n",
    "                    self.all_predictions.append(predictions.cpu())\n",
    "                    self.all_labels.append(true_labels.cpu())\n",
    "\n",
    "                # Ensure all tensors are on CPU before concatenation\n",
    "                self.all_predictions = torch.cat(self.all_predictions, dim=0)\n",
    "                self.all_labels = torch.cat(self.all_labels, dim=0)\n",
    "\n",
    "                self.plot_time_series(self.all_labels, self.all_predictions, trainer.current_epoch, trainer.datamodule.runoffDataStats)\n",
    "            pl_module.train()\n",
    "        except Exception as e:\n",
    "            print(f\"Exception in callback: {e}\")\n",
    "\n",
    "    def rolling_mean(self, data, window_size):\n",
    "            \"\"\"Apply a rolling mean to a 2D tensor along the time dimension.\"\"\"\n",
    "            cumsum_vec = np.cumsum(np.insert(data, 0, 0, axis=0), axis=0)\n",
    "            return (cumsum_vec[window_size:] - cumsum_vec[:-window_size]) / window_size\n",
    "\n",
    "    def plot_time_series(self, true_labels, predictions, epoch, runoffStats):\n",
    "        num_plots = min(len(predictions), self.num_samples_to_plot)\n",
    "        # Calculate the number of rows and columns for the subplot\n",
    "        num_cols = int(math.ceil(math.sqrt(num_plots)))\n",
    "        num_rows = int(math.ceil(num_plots / num_cols))\n",
    "\n",
    "        true_labels = true_labels*runoffStats[1].data + runoffStats[0].data\n",
    "        predictions = predictions*runoffStats[1].data + runoffStats[0].data\n",
    "\n",
    "        # Apply rolling mean\n",
    "        smoothed_true_labels = self.rolling_mean(true_labels.numpy(), self.rolling_window_size)\n",
    "        smoothed_predictions = self.rolling_mean(predictions.numpy(), self.rolling_window_size)\n",
    "\n",
    "        avg_labels = true_labels.mean(dim=0)\n",
    "        _, top_river_indices = torch.topk(avg_labels, self.num_samples_to_plot, largest=True)    \n",
    "\n",
    "        plt.figure(figsize=(num_cols * 4, num_rows * 4))  # Adjust the size dynamically based on the number of subplots\n",
    "        for i, river_index in enumerate(top_river_indices):\n",
    "            plt.subplot(num_rows, num_cols, i + 1)\n",
    "            plt.plot(smoothed_predictions[:, river_index], label=f\"Smoothed Pred for river {i}\")\n",
    "            plt.plot(smoothed_true_labels[:, river_index], label=f\"Smoothed True for river {i}\", alpha=0.5)\n",
    "            plt.legend()\n",
    "\n",
    "        plt.suptitle(f\"Epoch {epoch} Predictions {self.num_samples_to_plot} largest rivers\")  \n",
    "        plt.xlabel(\"Time\")  \n",
    "        plt.ylabel(\"Value\")  \n",
    "        plt.savefig(f'figures/time_series_predictions_epoch_{epoch}.png')\n",
    "        plt.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
