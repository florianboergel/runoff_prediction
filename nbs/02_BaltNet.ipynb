{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp BaltNet\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaltNet\n",
    "\n",
    "Model architecture used for predicting river runoff in the Baltic Sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning as L\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from BalticRiverPrediction.convLSTM import ConvLSTM\n",
    "from BalticRiverPrediction.sharedUtilities import EnhancedMSELoss, EnhancedMSEMetric\n",
    "from BalticRiverPrediction.sharedUtilities import PredictionPlottingCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaltNet(nn.Module):\n",
    "    def __init__(self, modelPar):\n",
    "        super(BaltNet, self).__init__()\n",
    "\n",
    "        # Initialize all attributes\n",
    "        for k, v in modelPar.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.encoder = ConvLSTM(\n",
    "            input_dim=self.input_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=self.batch_first,\n",
    "            bias=self.bias,\n",
    "            return_all_layers=False\n",
    "        )\n",
    "\n",
    "        self.decoder = ConvLSTM(\n",
    "            input_dim=self.hidden_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            num_layers=1,\n",
    "            batch_first=self.batch_first,\n",
    "            bias=self.bias,\n",
    "            return_all_layers=False\n",
    "        )\n",
    "\n",
    "        self.linear_dim = self.dimensions[0] * self.dimensions[1] * self.hidden_dim \n",
    "\n",
    "        # Single fully connected network for all rivers\n",
    "         \n",
    "        self.river_predictors = nn.Sequential(\n",
    "            nn.Linear(self.linear_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 97)\n",
    "        )\n",
    "\n",
    "        # Creating separate attention weights for each river\n",
    "        # self.attention_weights = nn.Parameter(torch.randn(self.hidden_dim, 1, 1), requires_grad=True)  # 97 rivers\n",
    "\n",
    "    # def spatial_attention(self, x):\n",
    "    #     \"\"\"Spatial attention mechanism.\"\"\"\n",
    "    #     B, T, C, H, W = x.size()\n",
    "\n",
    "    #     x = x.view(B * T, C, H, W)\n",
    "        \n",
    "    #     # Apply attention weights for all rivers\n",
    "    #     self.attention_map = torch.sigmoid(F.conv2d(x, self.attention_weights.unsqueeze(0), bias=None, stride=1, padding=0))\n",
    "        \n",
    "    #     # Weighted sum\n",
    "    #     output = x * self.attention_map  # B*T, C, H, W\n",
    "    #     output = output.view(B, T, C, H, W)  # B, T, C\n",
    "\n",
    "    #     return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, _, _, _ = x.size()\n",
    "\n",
    "        # Pass through encoder\n",
    "        encoder_outputs, encoder_hidden = self.encoder(x)\n",
    "\n",
    "        # Use the entire encoder output as input to the decoder\n",
    "        decoder_input = encoder_outputs[0][:,-1,:,:,:].unsqueeze(1)\n",
    "\n",
    "        # Pass through decoder using the final hidden state of the encoder\n",
    "        decoder_outputs, _ = self.decoder(decoder_input, encoder_hidden)\n",
    "\n",
    "        # Apply spatial attention\n",
    "        # decoder_with_spatial_attention = self.spatial_attention(decoder_outputs[0])  # B, T, C, H, W\n",
    "            \n",
    "        # Flatten the temporal sequence\n",
    "        decoder_with_spatial_attention_flattened = decoder_outputs[0].view(B, -1)  #\n",
    "            \n",
    "        # Pass through its own predictor\n",
    "        output = self.river_predictors(decoder_with_spatial_attention_flattened)  # B, -1\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BaltNet\n",
       "\n",
       ">      BaltNet (modelPar)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BaltNet\n",
       "\n",
       ">      BaltNet (modelPar)\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaltNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(BaltNet.spatial_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BaltNet.forward\n",
       "\n",
       ">      BaltNet.forward (x)\n",
       "\n",
       "Defines the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BaltNet.forward\n",
       "\n",
       ">      BaltNet.forward (x)\n",
       "\n",
       "Defines the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaltNet.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LightningModel(L.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning model for training and evaluation.\n",
    "    \n",
    "    Attributes:\n",
    "        model (nn.Module): The neural network model.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        cosine_t_max (int): Maximum number of iterations for the cosine annealing scheduler.\n",
    "        train_mse (torchmetrics.MeanSquaredError): Metric for training mean squared error.\n",
    "        val_mse (torchmetrics.MeanSquaredError): Metric for validation mean squared error.\n",
    "        test_mse (torchmetrics.MeanSquaredError): Metric for testing mean squared error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate, cosine_t_max, alpha=4):\n",
    "        \"\"\"\n",
    "        Initializes the LightningModel.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The neural network model.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "            cosine_t_max (int): Maximum number of iterations for the cosine annealing scheduler.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.cosine_t_max = cosine_t_max\n",
    "        self.loss_function = EnhancedMSELoss(alpha=alpha)\n",
    "\n",
    "        # Save hyperparameters except the model\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "        # Define metrics\n",
    "        self.train_mse = EnhancedMSEMetric(alpha=alpha)\n",
    "        self.val_mse = EnhancedMSEMetric(alpha=alpha)\n",
    "        self.test_mse = EnhancedMSEMetric(alpha=alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the model.\"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _shared_step(self, batch, debug=False):\n",
    "        \"\"\"\n",
    "        Shared step for training, validation, and testing.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): Input batch of data.\n",
    "            debug (bool, optional): If True, prints the loss. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Computed loss, true labels, and predicted labels.\n",
    "        \"\"\"\n",
    "        features, true_labels = batch\n",
    "        logits = self.model(features)\n",
    "        loss = self.loss_function(logits, true_labels)\n",
    "        if debug:\n",
    "            print(loss)\n",
    "        return loss, true_labels, logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        mse = self.train_mse(predicted_labels, true_labels)\n",
    "        metrics = {\"train_mse\": mse, \"train_loss\": loss}\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        mse = self.val_mse(predicted_labels, true_labels)\n",
    "        self.log(\"val_loss\", loss, sync_dist=True)\n",
    "        self.log(\"val_mse\", mse, prog_bar=True, sync_dist=True)\n",
    "    \n",
    "    def test_step(self, batch, _):\n",
    "        \"\"\"Test step.\"\"\"\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        mse = self.test_mse(predicted_labels, true_labels)\n",
    "        self.log(\"test_loss\", loss, rank_zero_only=True)\n",
    "        self.log(\"test_mse\", mse, sync_dist=True)\n",
    "        #return loss\n",
    "        return predicted_labels\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = 0):\n",
    "        \"\"\"Prediction step.\"\"\"\n",
    "        _, _, predicted_labels = self._shared_step(batch)\n",
    "        return predicted_labels\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer and learning rate scheduler.\n",
    "\n",
    "        Returns:\n",
    "            tuple: List of optimizers and list of learning rate schedulers.\n",
    "        \"\"\"\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10, verbose=False)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": sch, \"monitor\": \"val_mse\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightningModel\n",
       "\n",
       ">      LightningModel (model, learning_rate, cosine_t_max, alpha=4)\n",
       "\n",
       "A PyTorch Lightning model for training and evaluation.\n",
       "\n",
       "Attributes:\n",
       "    model (nn.Module): The neural network model.\n",
       "    learning_rate (float): Learning rate for the optimizer.\n",
       "    cosine_t_max (int): Maximum number of iterations for the cosine annealing scheduler.\n",
       "    train_mse (torchmetrics.MeanSquaredError): Metric for training mean squared error.\n",
       "    val_mse (torchmetrics.MeanSquaredError): Metric for validation mean squared error.\n",
       "    test_mse (torchmetrics.MeanSquaredError): Metric for testing mean squared error."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightningModel\n",
       "\n",
       ">      LightningModel (model, learning_rate, cosine_t_max, alpha=4)\n",
       "\n",
       "A PyTorch Lightning model for training and evaluation.\n",
       "\n",
       "Attributes:\n",
       "    model (nn.Module): The neural network model.\n",
       "    learning_rate (float): Learning rate for the optimizer.\n",
       "    cosine_t_max (int): Maximum number of iterations for the cosine annealing scheduler.\n",
       "    train_mse (torchmetrics.MeanSquaredError): Metric for training mean squared error.\n",
       "    val_mse (torchmetrics.MeanSquaredError): Metric for validation mean squared error.\n",
       "    test_mse (torchmetrics.MeanSquaredError): Metric for testing mean squared error."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightningModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LightningModel._shared_step\n",
       "\n",
       ">      LightningModel._shared_step (batch, debug=False)\n",
       "\n",
       "Shared step for training, validation, and testing.\n",
       "\n",
       "Args:\n",
       "    batch (tuple): Input batch of data.\n",
       "    debug (bool, optional): If True, prints the loss. Defaults to False.\n",
       "\n",
       "Returns:\n",
       "    tuple: Computed loss, true labels, and predicted labels."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LightningModel._shared_step\n",
       "\n",
       ">      LightningModel._shared_step (batch, debug=False)\n",
       "\n",
       "Shared step for training, validation, and testing.\n",
       "\n",
       "Args:\n",
       "    batch (tuple): Input batch of data.\n",
       "    debug (bool, optional): If True, prints the loss. Defaults to False.\n",
       "\n",
       "Returns:\n",
       "    tuple: Computed loss, true labels, and predicted labels."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LightningModel._shared_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AtmosphericDataset(Dataset):\n",
    "    def __init__(self, input_size, atmosphericData, runoff, transform=None):\n",
    "\n",
    "        # Length of the sequence\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # input data (x) \n",
    "        atmosphericDataStd = atmosphericData.std(\"time\") # dimension will be channel, lat, lon\n",
    "        atmosphericDataMean = atmosphericData.mean(\"time\")\n",
    "        self.atmosphericStats = (atmosphericDataMean, atmosphericDataStd)\n",
    "\n",
    "        # output data - label (y)\n",
    "        runoffData = runoff.transpose(\"time\", \"river\")\n",
    "        runoffDataMean = runoffData.mean(\"time\")\n",
    "        runoffDataSTD = runoffData.std(\"time\")\n",
    "        self.runoffDataStats = (runoffDataMean, runoffDataSTD)\n",
    "        \n",
    "        # normalize data\n",
    "        X = ((atmosphericData - atmosphericDataMean)/atmosphericDataStd).compute()\n",
    "        y = ((runoffData - runoffDataMean)/runoffDataSTD).compute()\n",
    "        \n",
    "        # an additional dimension for the channel is added\n",
    "        # to end up with (time, channel, lat, lon)\n",
    "        xStacked = X.to_array(dim='variable')\n",
    "        xStacked = xStacked.transpose(\"time\", \"variable\", \"y\", \"x\")\n",
    "\n",
    "        assert xStacked.data.ndim == 4\n",
    "        self.x = torch.tensor(xStacked.data, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.data, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index:index+(self.input_size)], self.y[index+int(self.input_size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]-(self.input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AtmosphericDataset\n",
       "\n",
       ">      AtmosphericDataset (input_size, atmosphericData, runoff, transform=None)\n",
       "\n",
       "An abstract class representing a :class:`Dataset`.\n",
       "\n",
       "All datasets that represent a map from keys to data samples should subclass\n",
       "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
       "data sample for a given key. Subclasses could also optionally overwrite\n",
       ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
       ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
       "of :class:`~torch.utils.data.DataLoader`.\n",
       "\n",
       ".. note::\n",
       "  :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
       "  sampler that yields integral indices.  To make it work with a map-style\n",
       "  dataset with non-integral indices/keys, a custom sampler must be provided."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AtmosphericDataset\n",
       "\n",
       ">      AtmosphericDataset (input_size, atmosphericData, runoff, transform=None)\n",
       "\n",
       "An abstract class representing a :class:`Dataset`.\n",
       "\n",
       "All datasets that represent a map from keys to data samples should subclass\n",
       "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
       "data sample for a given key. Subclasses could also optionally overwrite\n",
       ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
       ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
       "of :class:`~torch.utils.data.DataLoader`.\n",
       "\n",
       ".. note::\n",
       "  :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
       "  sampler that yields integral indices.  To make it work with a map-style\n",
       "  dataset with non-integral indices/keys, a custom sampler must be provided."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AtmosphericDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AtmosphereDataModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self, atmosphericData, runoff, batch_size=64, num_workers=8, add_first_dim=True, input_size=30):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = atmosphericData\n",
    "        self.runoff = runoff\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.add_first_dim = add_first_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def setup(self, stage:str):\n",
    "\n",
    "        UserWarning(\"Loading atmospheric data ...\")\n",
    "        dataset = AtmosphericDataset(\n",
    "            atmosphericData=self.data,\n",
    "            runoff=self.runoff,\n",
    "            input_size=self.input_size\n",
    "            )\n",
    "        n_samples = len(dataset)\n",
    "        train_size = int(0.8 * n_samples)  \n",
    "        val_size = int(0.1 * n_samples)   \n",
    "        test_size = n_samples - train_size - val_size  \n",
    "\n",
    "        generator1 = torch.Generator().manual_seed(42)\n",
    "        self.train, self.val, self.test = random_split(dataset, [train_size, val_size, test_size], generator=generator1)\n",
    "        self.runoffDataStats = dataset.runoffDataStats\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True, \n",
    "            drop_last=True, \n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=False  # Speed up data transfer to GPU\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            drop_last=True,\n",
    "            pin_memory=False  # Speed up data transfer to GPU\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            drop_last=True,\n",
    "            pin_memory=False  # Speed up data transfer to GPU\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### AtmosphereDataModule\n",
       "\n",
       ">      AtmosphereDataModule (atmosphericData, runoff, batch_size=64,\n",
       ">                            num_workers=8, add_first_dim=True, input_size=30)\n",
       "\n",
       "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
       "consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    class MyDataModule(LightningDataModule):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "        def prepare_data(self):\n",
       "            # download, split, etc...\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "        def train_dataloader(self):\n",
       "            train_split = Dataset(...)\n",
       "            return DataLoader(train_split)\n",
       "        def val_dataloader(self):\n",
       "            val_split = Dataset(...)\n",
       "            return DataLoader(val_split)\n",
       "        def test_dataloader(self):\n",
       "            test_split = Dataset(...)\n",
       "            return DataLoader(test_split)\n",
       "        def teardown(self):\n",
       "            # clean up after fit or test\n",
       "            # called on every process in DDP"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### AtmosphereDataModule\n",
       "\n",
       ">      AtmosphereDataModule (atmosphericData, runoff, batch_size=64,\n",
       ">                            num_workers=8, add_first_dim=True, input_size=30)\n",
       "\n",
       "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
       "consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    class MyDataModule(LightningDataModule):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "        def prepare_data(self):\n",
       "            # download, split, etc...\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "        def train_dataloader(self):\n",
       "            train_split = Dataset(...)\n",
       "            return DataLoader(train_split)\n",
       "        def val_dataloader(self):\n",
       "            val_split = Dataset(...)\n",
       "            return DataLoader(val_split)\n",
       "        def test_dataloader(self):\n",
       "            test_split = Dataset(...)\n",
       "            return DataLoader(test_split)\n",
       "        def teardown(self):\n",
       "            # clean up after fit or test\n",
       "            # called on every process in DDP"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AtmosphereDataModule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
